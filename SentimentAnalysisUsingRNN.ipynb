{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Integer Encoding"
      ],
      "metadata": {
        "id": "POvbDxRE2Egh"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Importing all the necessary libraries, including the IMDB Datasets, RNN layers, Embeddings and Dense layers."
      ],
      "metadata": {
        "id": "Gbeb_YTRgWJk"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "dU4NE8bBgSJx"
      },
      "outputs": [],
      "source": [
        "from keras.datasets import imdb\n",
        "from keras.layers import SimpleRNN, Dense\n",
        "from keras import Sequential\n",
        "from keras.preprocessing.sequence import pad_sequences"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Splitting the dataset into test and train, and viewing how the vector representation of the data looks like"
      ],
      "metadata": {
        "id": "NP-wfT5whRRA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "(x_train,y_train),(x_test,y_test)=imdb.load_data()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkfAj12hh2SZ",
        "outputId": "a746dd6e-53da-45d0-ceae-a865d6788b97"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/imdb.npz\n",
            "17464789/17464789 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "x_train"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x7QvzZDtigJC",
        "outputId": "19f436fe-ce1a-40a1-c139-01954a81a897"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([list([1, 14, 22, 16, 43, 530, 973, 1622, 1385, 65, 458, 4468, 66, 3941, 4, 173, 36, 256, 5, 25, 100, 43, 838, 112, 50, 670, 22665, 9, 35, 480, 284, 5, 150, 4, 172, 112, 167, 21631, 336, 385, 39, 4, 172, 4536, 1111, 17, 546, 38, 13, 447, 4, 192, 50, 16, 6, 147, 2025, 19, 14, 22, 4, 1920, 4613, 469, 4, 22, 71, 87, 12, 16, 43, 530, 38, 76, 15, 13, 1247, 4, 22, 17, 515, 17, 12, 16, 626, 18, 19193, 5, 62, 386, 12, 8, 316, 8, 106, 5, 4, 2223, 5244, 16, 480, 66, 3785, 33, 4, 130, 12, 16, 38, 619, 5, 25, 124, 51, 36, 135, 48, 25, 1415, 33, 6, 22, 12, 215, 28, 77, 52, 5, 14, 407, 16, 82, 10311, 8, 4, 107, 117, 5952, 15, 256, 4, 31050, 7, 3766, 5, 723, 36, 71, 43, 530, 476, 26, 400, 317, 46, 7, 4, 12118, 1029, 13, 104, 88, 4, 381, 15, 297, 98, 32, 2071, 56, 26, 141, 6, 194, 7486, 18, 4, 226, 22, 21, 134, 476, 26, 480, 5, 144, 30, 5535, 18, 51, 36, 28, 224, 92, 25, 104, 4, 226, 65, 16, 38, 1334, 88, 12, 16, 283, 5, 16, 4472, 113, 103, 32, 15, 16, 5345, 19, 178, 32]),\n",
              "       list([1, 194, 1153, 194, 8255, 78, 228, 5, 6, 1463, 4369, 5012, 134, 26, 4, 715, 8, 118, 1634, 14, 394, 20, 13, 119, 954, 189, 102, 5, 207, 110, 3103, 21, 14, 69, 188, 8, 30, 23, 7, 4, 249, 126, 93, 4, 114, 9, 2300, 1523, 5, 647, 4, 116, 9, 35, 8163, 4, 229, 9, 340, 1322, 4, 118, 9, 4, 130, 4901, 19, 4, 1002, 5, 89, 29, 952, 46, 37, 4, 455, 9, 45, 43, 38, 1543, 1905, 398, 4, 1649, 26, 6853, 5, 163, 11, 3215, 10156, 4, 1153, 9, 194, 775, 7, 8255, 11596, 349, 2637, 148, 605, 15358, 8003, 15, 123, 125, 68, 23141, 6853, 15, 349, 165, 4362, 98, 5, 4, 228, 9, 43, 36893, 1157, 15, 299, 120, 5, 120, 174, 11, 220, 175, 136, 50, 9, 4373, 228, 8255, 5, 25249, 656, 245, 2350, 5, 4, 9837, 131, 152, 491, 18, 46151, 32, 7464, 1212, 14, 9, 6, 371, 78, 22, 625, 64, 1382, 9, 8, 168, 145, 23, 4, 1690, 15, 16, 4, 1355, 5, 28, 6, 52, 154, 462, 33, 89, 78, 285, 16, 145, 95]),\n",
              "       list([1, 14, 47, 8, 30, 31, 7, 4, 249, 108, 7, 4, 5974, 54, 61, 369, 13, 71, 149, 14, 22, 112, 4, 2401, 311, 12, 16, 3711, 33, 75, 43, 1829, 296, 4, 86, 320, 35, 534, 19, 263, 4821, 1301, 4, 1873, 33, 89, 78, 12, 66, 16, 4, 360, 7, 4, 58, 316, 334, 11, 4, 1716, 43, 645, 662, 8, 257, 85, 1200, 42, 1228, 2578, 83, 68, 3912, 15, 36, 165, 1539, 278, 36, 69, 44076, 780, 8, 106, 14, 6905, 1338, 18, 6, 22, 12, 215, 28, 610, 40, 6, 87, 326, 23, 2300, 21, 23, 22, 12, 272, 40, 57, 31, 11, 4, 22, 47, 6, 2307, 51, 9, 170, 23, 595, 116, 595, 1352, 13, 191, 79, 638, 89, 51428, 14, 9, 8, 106, 607, 624, 35, 534, 6, 227, 7, 129, 113]),\n",
              "       ...,\n",
              "       list([1, 11, 6, 230, 245, 6401, 9, 6, 1225, 446, 86527, 45, 2174, 84, 8322, 4007, 21, 4, 912, 84, 14532, 325, 725, 134, 15271, 1715, 84, 5, 36, 28, 57, 1099, 21, 8, 140, 8, 703, 5, 11656, 84, 56, 18, 1644, 14, 9, 31, 7, 4, 9406, 1209, 2295, 26094, 1008, 18, 6, 20, 207, 110, 563, 12, 8, 2901, 17793, 8, 97, 6, 20, 53, 4767, 74, 4, 460, 364, 1273, 29, 270, 11, 960, 108, 45, 40, 29, 2961, 395, 11, 6, 4065, 500, 7, 14492, 89, 364, 70, 29, 140, 4, 64, 4780, 11, 4, 2678, 26, 178, 4, 529, 443, 17793, 5, 27, 710, 117, 74936, 8123, 165, 47, 84, 37, 131, 818, 14, 595, 10, 10, 61, 1242, 1209, 10, 10, 288, 2260, 1702, 34, 2901, 17793, 4, 65, 496, 4, 231, 7, 790, 5, 6, 320, 234, 2766, 234, 1119, 1574, 7, 496, 4, 139, 929, 2901, 17793, 7750, 5, 4241, 18, 4, 8497, 13164, 250, 11, 1818, 7561, 4, 4217, 5408, 747, 1115, 372, 1890, 1006, 541, 9303, 7, 4, 59, 11027, 4, 3586, 22459]),\n",
              "       list([1, 1446, 7079, 69, 72, 3305, 13, 610, 930, 8, 12, 582, 23, 5, 16, 484, 685, 54, 349, 11, 4120, 2959, 45, 58, 1466, 13, 197, 12, 16, 43, 23, 21469, 5, 62, 30, 145, 402, 11, 4131, 51, 575, 32, 61, 369, 71, 66, 770, 12, 1054, 75, 100, 2198, 8, 4, 105, 37, 69, 147, 712, 75, 3543, 44, 257, 390, 5, 69, 263, 514, 105, 50, 286, 1814, 23, 4, 123, 13, 161, 40, 5, 421, 4, 116, 16, 897, 13, 40691, 40, 319, 5872, 112, 6700, 11, 4803, 121, 25, 70, 3468, 4, 719, 3798, 13, 18, 31, 62, 40, 8, 7200, 4, 29455, 7, 14, 123, 5, 942, 25, 8, 721, 12, 145, 5, 202, 12, 160, 580, 202, 12, 6, 52, 58, 11418, 92, 401, 728, 12, 39, 14, 251, 8, 15, 251, 5, 21213, 12, 38, 84, 80, 124, 12, 9, 23]),\n",
              "       list([1, 17, 6, 194, 337, 7, 4, 204, 22, 45, 254, 8, 106, 14, 123, 4, 12815, 270, 14437, 5, 16923, 12255, 732, 2098, 101, 405, 39, 14, 1034, 4, 1310, 9, 115, 50, 305, 12, 47, 4, 168, 5, 235, 7, 38, 111, 699, 102, 7, 4, 4039, 9245, 9, 24, 6, 78, 1099, 17, 2345, 16553, 21, 27, 9685, 6139, 5, 29043, 1603, 92, 1183, 4, 1310, 7, 4, 204, 42, 97, 90, 35, 221, 109, 29, 127, 27, 118, 8, 97, 12, 157, 21, 6789, 85010, 9, 6, 66, 78, 1099, 4, 631, 1191, 5, 2642, 272, 191, 1070, 6, 7585, 8, 2197, 70907, 10755, 544, 5, 383, 1271, 848, 1468, 12183, 497, 16876, 8, 1597, 8778, 19280, 21, 60, 27, 239, 9, 43, 8368, 209, 405, 10, 10, 12, 764, 40, 4, 248, 20, 12, 16, 5, 174, 1791, 72, 7, 51, 6, 1739, 22, 4, 204, 131, 9])],\n",
              "      dtype=object)"
            ]
          },
          "metadata": {},
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2tPQTgQ3jnOw",
        "outputId": "f136e7be-5d32-4ef4-a83f-8dbcc8defbf9"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train[0])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CWTI0wX184Zc",
        "outputId": "9fbcbed7-7fc9-4e21-d673-ab5e1c3e0b8f"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "218"
            ]
          },
          "metadata": {},
          "execution_count": 14
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train[278])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WbAoEuIV8-uX",
        "outputId": "bbd1edce-add4-423c-c342-a3039239a344"
      },
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "140"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=0\n",
        "for i,review in enumerate(x_train):\n",
        "  if len(x_train[i])>max_len:\n",
        "    #print(review)\n",
        "    max_len=len(x_train[i])\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wOs54VutjJ-O",
        "outputId": "cbc2973f-7c98-42d7-a6af-190b89160b6f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now, we have the maximum lenght of review, which we will use to pad the shorter sequences befre feeding into the RNN layer"
      ],
      "metadata": {
        "id": "9z3FPlTm7JR_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "x_train=pad_sequences(x_train, padding='post',maxlen=max_len)\n",
        "x_test=pad_sequences(x_test,padding='post',maxlen=max_len)"
      ],
      "metadata": {
        "id": "O8JskGez7Hp9"
      },
      "execution_count": 19,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Now our integer coded embeddings are ready, that is all sequences are of the same lenght which can now be fed into our model"
      ],
      "metadata": {
        "id": "YW2Ve_lb-FSP"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(x_train[0])\n",
        "len(x_train[278])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lUj8cFdU-Pfo",
        "outputId": "4dffc567-d75b-4772-b6bb-393c06c862e1"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "2494"
            ]
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model = Sequential()\n",
        "model.add(SimpleRNN(32, input_shape=(max_len,1), return_sequences=False))\n",
        "model.add(Dense(1, activation='sigmoid'))\n",
        "model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FIchcVM2-vrk",
        "outputId": "130c456c-56a2-478d-9522-696a2f00cf99"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " simple_rnn_3 (SimpleRNN)    (None, 32)                1088      \n",
            "                                                                 \n",
            " dense_2 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 1121 (4.38 KB)\n",
            "Trainable params: 1121 (4.38 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "We have 1088 trainable parameters, that is 32 weights of the current time step plus 32*32 from the previous time plus 32 biases which gives a total of 1088 parameters in the RNN layer."
      ],
      "metadata": {
        "id": "h3q6hc5x_Z9H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "model.compile(loss='binary_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
        "\n",
        "model.fit(x_train,y_train,epochs=5,validation_data=(x_test,y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ymflPomy_5H6",
        "outputId": "3160470f-c1cf-4e39-d7b0-2cb801093eaa"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 329s 419ms/step - loss: 0.6950 - accuracy: 0.5037 - val_loss: 0.7001 - val_accuracy: 0.4936\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 335s 428ms/step - loss: 0.6962 - accuracy: 0.4920 - val_loss: 0.6933 - val_accuracy: 0.5028\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 337s 432ms/step - loss: 0.6937 - accuracy: 0.4986 - val_loss: 0.6935 - val_accuracy: 0.5043\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 325s 416ms/step - loss: 0.6937 - accuracy: 0.5020 - val_loss: 0.6939 - val_accuracy: 0.5029\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 328s 419ms/step - loss: 0.6938 - accuracy: 0.5022 - val_loss: 0.6934 - val_accuracy: 0.4995\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.History at 0x791f932c73a0>"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Embeddings Encoding"
      ],
      "metadata": {
        "id": "8BXD3rVV12wA"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "In the above code, we created the numerical vector from text using integer encoding. Now, we will create the numerical vector using embeddings:"
      ],
      "metadata": {
        "id": "vWQ9_GRUvEl9"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "The diffrenece is that, when we use interger encoding, the lenght of each numerical vector is the same as the lenght of the review that is the number of words in review. As RNN is designed to be trained on a fixed lenght input, we had to perform padding. This resulted in a **Sparse vector**, i.e. a vector with zeroes  along with actual integer encondings.\n",
        "If we use embeddings, the embedded vector size can be initialised, which will hold the contexual meaning of the sentence.\\\n",
        "Steps:\\\n",
        "1) Pad the text sequence to a fixed lenght (n), so that all the rows are of same lenght.\\\n",
        "2) Initialise the the number of nodes for the embedding layer in the Neural network.\\\n",
        "3) Specify the output dimension (d) of the embedding vector.\\\n",
        "4)Predict the sequences from the text: Each sequence will be a list of size n, where each word is an embedded vector of size d.\\\n",
        "This results in creating a **Dense vector**."
      ],
      "metadata": {
        "id": "wwj1uGhbvaWC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from keras.datasets import imdb\n",
        "from keras import Sequential\n",
        "from keras.layers import Flatten, Embedding, SimpleRNN, Dense\n",
        "from keras.utils import pad_sequences"
      ],
      "metadata": {
        "id": "60mlB1A6x1ce"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "(X_train,Y_train),(X_test,Y_test)=imdb.load_data(num_words=10000, oov_char=0)"
      ],
      "metadata": {
        "id": "TtzXhz7_4Lec"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "len(X_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JjlDDbpA4V-0",
        "outputId": "c8fbd2c6-f5f1-471f-8771-fa9c8cbc02f0"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "25000"
            ]
          },
          "metadata": {},
          "execution_count": 34
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4kBF9Dui47FQ",
        "outputId": "83671cc3-a34d-4220-ad3d-c124f0c4e56a"
      },
      "execution_count": 35,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000,)"
            ]
          },
          "metadata": {},
          "execution_count": 35
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_len=0\n",
        "for i,review in enumerate(X_train):\n",
        "  if len(X_train[i])>max_len:\n",
        "    #print(review)\n",
        "    max_len=len(X_train[i])\n",
        "print(max_len)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7BhaocHwTT_2",
        "outputId": "fa2d9703-eb13-4082-c959-63e2468b889f"
      },
      "execution_count": 36,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "2494\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "X_train=pad_sequences(X_train, padding='post', maxlen=2494)\n",
        "X_test=pad_sequences(X_test, padding='post', maxlen=2494)"
      ],
      "metadata": {
        "id": "H6yRPjs8TFu0"
      },
      "execution_count": 37,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2M04PCuD6V2",
        "outputId": "a968ed33-4f19-4133-9ae1-bcbb6a59aabc"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(25000, 2494)"
            ]
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2 = Sequential()\n",
        "model2.add(Embedding(10000, 10,input_length=2494))\n",
        "model2.add(SimpleRNN(32,return_sequences=False))\n",
        "model2.add(Dense(1, activation='sigmoid'))\n",
        "\n",
        "model2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QhC_cvY_5AXF",
        "outputId": "b1372c27-444a-4fde-b79d-d3b62dfae24e"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model: \"sequential_6\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " embedding_6 (Embedding)     (None, 2494, 10)          100000    \n",
            "                                                                 \n",
            " simple_rnn_5 (SimpleRNN)    (None, 32)                1376      \n",
            "                                                                 \n",
            " dense_5 (Dense)             (None, 1)                 33        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 101409 (396.13 KB)\n",
            "Trainable params: 101409 (396.13 KB)\n",
            "Non-trainable params: 0 (0.00 Byte)\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "model2.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])\n",
        "history = model2.fit(X_train, Y_train,epochs=5,validation_data=(X_test,Y_test))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X4xdqvIm53C5",
        "outputId": "eb501a4c-ed3b-42d8-9d09-db4080e3318b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 1005s 1s/step - loss: 0.6964 - accuracy: 0.5001 - val_loss: 0.6935 - val_accuracy: 0.5040\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 986s 1s/step - loss: 0.6940 - accuracy: 0.4975 - val_loss: 0.6932 - val_accuracy: 0.5084\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 1052s 1s/step - loss: 0.6943 - accuracy: 0.4970 - val_loss: 0.6939 - val_accuracy: 0.5000\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 1168s 1s/step - loss: 0.6943 - accuracy: 0.4952 - val_loss: 0.6949 - val_accuracy: 0.4914\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 1081s 1s/step - loss: 0.6949 - accuracy: 0.5050 - val_loss: 0.6936 - val_accuracy: 0.5000\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reasons for lower accuracy:\\\n",
        "1) The IMDB dataset used, was encoded using integer encoding, where each index was represented by a number which is the frequency of that word in the corpus, that is the number of times that word appeared in all the reviews.\n",
        "While defining the input dimension of the embedding layer, we we specified the word index as 10000, while there is a much bigger count of thw word index in the dataset. Hence the model did not get trained properly, and it got trained on a highly sparse dataset.\\\n",
        "2) We trained the model only with epochs=5, training for a longer time could have resulted in weights capturing more insights through backpropagation, resulting in better accuracy.\\\n",
        "3) Tweaking the number of nodes in the RNN layer.\\\n",
        "4) Incraesing the timesteps in context window of the RNN model."
      ],
      "metadata": {
        "id": "TeJq-k9pvxZR"
      }
    }
  ]
}